{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 나이브 베이즈 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이브 베이즈(naive bayes) 분류기는 선형 모델과 매우 유사\n",
    "# LogisticRegression, LinearSVC 같은 선형 분류기보다 훈련 속도가 빠르지만\n",
    "# 일반화 성능이 조금 뒤진다.\n",
    "\n",
    "# 나이브 베이즈 분류기가 효과적인 이유\n",
    "#  - 각 특성을 개별로 취급해 파라미터 학습\n",
    "#  - 각 특성에서 클래스별 통계를 단순하게 취합\n",
    "\n",
    "# 나이브 베이즈 분류기 종류\n",
    "# 1) GaussianNB: 연속적인 데이터에 적용\n",
    "# 2) BernoulliNB: 이진 데이터(이산적인 데이터)에 적용\n",
    "# 3) MultinomialVN: 카운트 데이터(이산적인 데이터)에 적용\n",
    "#    * 카운트 데이터: 특성이 어떤 것을 헤아린 정수 카운트\n",
    "#        예) 문장에 나타난 단어의 횟수\n",
    "# *) BernoulliNB, MultinomialNB는 대부분 텍스트 데이터 분류 시 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. BernoulliNB 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 카운트:\n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스의 특성 중 0이 아닌 것이 몇 개인지 센다.\n",
    "\n",
    "# 이진 특성을 4개 가진 데이터 포인트가 4개\n",
    "# 클래스 : 0, 1\n",
    "# 출력 y\n",
    "X = np.array([[0, 1, 0, 1],\n",
    "             [1, 0, 1, 1],\n",
    "             [0, 0, 0, 1],\n",
    "             [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # 클래스마다 반복\n",
    "    # 특성값이 1이 나타난 횟수를 센다.\n",
    "    counts[label] = X[y == label].sum(axis=0) # axis=0은 행을 따라\n",
    "\n",
    "print(\"특성 카운트:\\n{}\".format(counts))\n",
    "\n",
    "# 해당 클래스의 데이터 포인트(행)에서 각 특성(열)별 1의 개수를 센다.\n",
    "# 클래스별 특성 수 세기 = 클래스별 0이 아닌 원소 세기\n",
    "# 클래스 0인 특성 수           클래스 1인 특성 수\n",
    "#     1행: 0 1 0 1                2행: 1 0 1 1\n",
    "#   + 3행: 0 0 0 1              + 4행: 1 0 1 0\n",
    "# ---------------------        ----------------------\n",
    "# 특성 수: 0 1 0 2            특성 수: 2 0 2 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나머지 두 나이브 베이즈 모델은 계산하는 통계 데이터 종류가 다름\n",
    "# MultinomialNB: 클래스별로 특성의 평균 저장\n",
    "# GaussianNB: 클래스별로 각 특성의 표준편차와 평균 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 예측 시 나이브 베이즈 분류기 메커니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 시 데이터 포인트를 클래스의 통계 값과 비교해서 가장 잘 맞는 클래스를\n",
    "# 예측값으로 사용\n",
    "\n",
    "# MultinomialNB, BernoulliNB 예측 공식: 선형 모델과 형태가 동일\n",
    "# 이 때, coef_: 기울기(w)가 아닌 특성 카운트 수를 로그 변환한 형태\n",
    "# intercept_: 클래스 카운트 수를 로그 변환한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 나이브 베이즈 분류기 장단점과 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB, BernoulliNB\n",
    "# 텍스트 같은 희소한 데이터 카운트에 사용\n",
    "# MultimomialNB는 보통 0이 아닌 특성이 비교적 많은 데이터셋(예를 들어 큰 문서들)에서\n",
    "# BernoulliNB보다 성능이 높다.\n",
    "# 매개변수: 모델의 복잡도를 조절하는 alpha\n",
    "# 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 alpha 개수만큼 추가\n",
    "# ⇒ 통계 데이터를 완만하게 만들어준다.\n",
    "# 즉 alpha가 크면 더 완만해지고, 복잡도는 낮아짐\n",
    "# alpha에 따른 알고리즘 성능 변동이 비교적 크지않아, \n",
    "# alpha값이 성능 향상에 크게 기여하지 않음\n",
    "# 그러나 어느 정도는 정확도를 높일 수 있음\n",
    "\n",
    "# GaussianNB\n",
    "# 대부분 매우 고차원 데이터셋에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 선형 모델과의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이브 베이즈 모델은 선형 모델의 장단점과 비슷\n",
    "# 훈련, 예측 속도가 빠름\n",
    "# 훈련 과정 이해가 쉬움\n",
    "# 희소한 고차원 데이터에서 잘 작동\n",
    "# 비교적 매개변수에 민감하지 않음\n",
    "# 선형 모델로 학습 시간이 너무 오래 걸리는 매우 큰 데이터셋에\n",
    "# 나이브 베이즈 모델이 종종 사용된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
